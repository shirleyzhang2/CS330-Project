{
    "Contributors": [
        "Yeganeh Kordi"
    ],
    "Source": [
        "qasper"
    ],
    "URL": [
        "https://allenai.org/project/qasper/home"
    ],
    "Categories": [
        "Question Understanding"
    ],
    "Reasoning": [],
    "Definition": [
        "In this task, you will be presented with a context from an academic paper and a question based on the context. Your task is to classify the questions into \"Extractive\", \"Abstractive\", or \"Yes-no\" questions. Extractive questions are questions that can be answered by concatenating extracts taken from a context into a summary. Abstractive questions are questions that require paraphrasing the context using novel sentences. Yes-no questions are questions whose expected answer is one of two choices, one that affirms the question and one that denies the question. Typically, the choices are either yes or no."
    ],
    "Input_language": [
        "English"
    ],
    "Output_language": [
        "English"
    ],
    "Instruction_language": [
        "English"
    ],
    "Domains": [
        "Scientific Research Papers"
    ],
    "Positive Examples": [
        {
            "input": "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. \n Question: How did they obtain the dataset?",
            "output": "Extractive",
            "explanation": "The answer to this question has been explicitly mentioned in the context, so the question is extractive."
        },
        {
            "input": "We evaluate our pointer-generator performance using BLEU score. The baseline language model is trained using RNNLM BIBREF23 . Perplexity measure is used in the evaluation.\n\n \n Question: Did they use other evaluation metrics?",
            "output": "Yes-no",
            "explanation": "This is a good example, and the given question is a yes-no question."
        },
        {
            "input": "As in the example above, we pre-process documents by removing all numbers and interjections. \n Question: what processing was done on the speeches before being parsed?",
            "output": "Abstractive",
            "explanation": "Answering this question needs paraphrasing the context, so this question is abstractive."
        }
    ],
    "Negative Examples": [
        {
            "input": "For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains  For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains  The evaluation of the German version is in progress. \n Question: What are the corpora used for the task?",
            "output": "Yes-no",
            "explanation": "This question is extractive, and it's not a yes-no question."
        },
        {
            "input": "For Turkish, as the first dataset, we utilised the movie reviews which are collected from a popular website. The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment. These polarity scores are between the values 0.5 and 5, at intervals of 0.5. We consider a review to be negative it the score is equal to or lower than 2.5. On the other hand, if it is equal to or higher than 4, it is assumed to be positive. We have randomly selected 7,020 negative and 7,020 positive reviews and processed only them. \n Question: What details are given about the movie domain dataset?",
            "output": "Extractive",
            "explanation": "This question is Abstractive because we need to paraphrase the context to get to the answer."
        }
    ],
    "Instances": [
        {
            "id": "task462-a0a13f4db382431a8229adc7a4cba10a",
            "input": "To assess the predictive capability of this and other models, we require some method by which we can compare the models. For that purpose, we use receiver operating characteristic (ROC) curves as a visual representation of predictive effectiveness. ROC curves compare the true positive rate (TPR) and false positive rate (FPR) of a model's predictions at different threshold levels. The area under the curve (AUC) (between 0 and 1) is a numerical measure, where the higher the AUC is, the better the model performs. We cross-validate our model by first randomly splitting the corpus into a training set (95% of the corpus) and test set (5% of the corpus). We then fit the model to the training set, and use it to predict the response of the documents in the test set. We repeat this process 100 times. The threshold-averaged ROC curve BIBREF13 is found from these predictions, and shown in Figure 3 . Table 1 shows the AUC for each model considered. \n Question: How is performance measured?",
            "output": [
                "Abstractive"
            ]
        },
        {
            "id": "task462-8f90c23145ae4c31b2e516094381de1c",
            "input": "Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 \n Question: What does KBQA abbreviate for",
            "output": [
                "Extractive"
            ]
        },
        {
            "id": "task462-3135328725fd4e80a427ec20f175ed95",
            "input": "Figure FIGREF5 shows that E-BERT performs comparable to BERT and ERNIE on unfiltered LAMA. \n Question: Which of the two ensembles yields the best performance?",
            "output": [
                "Abstractive"
            ]
        },
        {
            "id": "task462-7a40d6ac207c4b7ba83ea4b55e902bf5",
            "input": "Table TABREF15 shows a comparison of the results on SimCluster versus K-means algorithm. Here our SimCluster algorithm improves the F1-scores from 0.412 and 0.417 in the two domains to 0.442 and 0.441. The ARI scores also improve from 0.176 and 0.180 to 0.203 and 0.204. \n Question: Do they use the same distance metric for both the SimCluster and K-means algorithm?",
            "output": [
                "Yes-no"
            ]
        },
        {
            "id": "task462-2aed0842a5914f2a9057d65a6b5f68de",
            "input": "For each candidate mention span, we retrieve the top 10 entities according to the Freebase API. We then create a lattice in which the nodes correspond to mention-entity pairs, scored by their Freebase API scores, and the edges encode the fact that no joint assignment of entities to mentions can contain overlapping spans. We take the top 10 paths through the lattice as possible entity disambiguations. For each possibility, we generate $n$ -best paraphrases that contains the entity mention spans. In the end, this process creates a total of $10n$ paraphrases.  \n Question: How many paraphrases are generated per question?",
            "output": [
                "Abstractive"
            ]
        },
        {
            "id": "task462-9ddec1c934f44676bbcd762d2240b462",
            "input": "RKS approach proposed in BIBREF21, BIBREF22, explicitly maps data vectors to a space where linear separation is possible. The RKS method provides an approximate kernel function via explicit mapping. \n Question: What is the Random Kitchen Sink approach?",
            "output": [
                "Abstractive"
            ]
        },
        {
            "id": "task462-c064083381cc4ee4b23f8b1953f39e53",
            "input": "Multi-task Pairwise Neural Ranking\nWe propose a multi-task pairwise neural ranking approach to better incorporate and distinguish the relative order between the candidate segmentations of a given hashtag. Our model adapts to address single- and multi-token hashtags differently via a multi-task learning strategy without requiring additional annotations. In this section, we describe the task setup and three variants of pairwise neural ranking models (Figure FIGREF11 ). Pairwise Neural Ranking Model Margin Ranking (MR) Loss Adaptive Multi-task Learning \n Question: What set of approaches to hashtag segmentation are proposed?",
            "output": [
                "Extractive"
            ]
        },
        {
            "id": "task462-506443df67a548a99c052c609085f785",
            "input": "In the second round, we collected 293 annotations from 12 annotators. After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. \n Question: How are possible sentence transformations represented in dataset, as new sentences?",
            "output": [
                "Abstractive"
            ]
        }
    ],
    "Instance License": [
        "CC BY 4.0"
    ]
}