{
    "predict_exact_match_for_task065_timetravel_consistent_sentence_classification": 59.0,
    "predict_exact_match_for_task066_timetravel_binary_consistency_classification": 54.0,
    "predict_exact_match_for_task069_abductivenli_classification": 45.0,
    "predict_exact_match_for_task070_abductivenli_incorrect_classification": 52.0,
    "predict_exact_match_for_task092_check_prime_classification": 63.0,
    "predict_exact_match_for_task108_contextualabusedetection_classification": 77.0,
    "predict_exact_match_for_task1135_xcsr_en_commonsense_mc_classification": 19.0,
    "predict_exact_match_for_task115_help_advice_classification": 61.0,
    "predict_exact_match_for_task1186_nne_hrngo_classification": 49.0,
    "predict_exact_match_for_task1193_food_course_classification": 45.0,
    "predict_exact_match_for_task1196_atomic_classification_oeffect": 56.0,
    "predict_exact_match_for_task1197_atomic_classification_oreact": 70.0,
    "predict_exact_match_for_task1198_atomic_classification_owant": 62.0,
    "predict_exact_match_for_task1199_atomic_classification_xattr": 56.0,
    "predict_exact_match_for_task1200_atomic_classification_xeffect": 67.0,
    "predict_exact_match_for_task1201_atomic_classification_xintent": 66.0,
    "predict_exact_match_for_task1202_atomic_classification_xneed": 62.0,
    "predict_exact_match_for_task1203_atomic_classification_xreact": 60.0,
    "predict_exact_match_for_task1204_atomic_classification_hinderedby": 44.0,
    "predict_exact_match_for_task1205_atomic_classification_isafter": 48.0,
    "predict_exact_match_for_task1206_atomic_classification_isbefore": 38.0,
    "predict_exact_match_for_task1207_atomic_classification_atlocation": 50.0,
    "predict_exact_match_for_task1208_atomic_classification_xreason": 39.0,
    "predict_exact_match_for_task1210_atomic_classification_madeupof": 49.0,
    "predict_exact_match_for_task1212_atomic_classification_hasproperty": 43.0,
    "predict_exact_match_for_task1214_atomic_classification_xwant": 63.0,
    "predict_exact_match_for_task1216_atomic_classification_causes": 52.0,
    "predict_exact_match_for_task1283_hrngo_quality_classification": 57.0,
    "predict_exact_match_for_task1284_hrngo_informativeness_classification": 66.0,
    "predict_exact_match_for_task1289_trec_classification": 36.0,
    "predict_exact_match_for_task1308_amazonreview_category_classification": 59.0,
    "predict_exact_match_for_task1309_amazonreview_summary_classification": 53.0,
    "predict_exact_match_for_task1310_amazonreview_rating_classification": 25.0,
    "predict_exact_match_for_task1311_amazonreview_rating_classification": 35.0,
    "predict_exact_match_for_task1312_amazonreview_polarity_classification": 61.0,
    "predict_exact_match_for_task1313_amazonreview_polarity_classification": 51.0,
    "predict_exact_match_for_task1341_msr_text_classification": 54.0,
    "predict_exact_match_for_task1346_glue_cola_grammatical_correctness_classification": 57.0,
    "predict_exact_match_for_task1347_glue_sts-b_similarity_classification": 18.0,
    "predict_exact_match_for_task1354_sent_comp_classification": 60.0,
    "predict_exact_match_for_task1361_movierationales_classification": 99.0,
    "predict_exact_match_for_task1366_healthfact_classification": 47.0,
    "predict_exact_match_for_task137_detoxifying-lms_classification_toxicity": 53.0,
    "predict_exact_match_for_task1384_deal_or_no_dialog_classification": 77.0,
    "predict_exact_match_for_task139_detoxifying-lms_classification_topicality": 51.0,
    "predict_exact_match_for_task140_detoxifying-lms_classification_style": 46.0,
    "predict_exact_match_for_task1418_bless_semantic_relation_classification": 9.0,
    "predict_exact_match_for_task141_odd-man-out_classification_category": 26.0,
    "predict_exact_match_for_task1429_evalution_semantic_relation_classification": 1.0,
    "predict_exact_match_for_task142_odd-man-out_classification_no_category": 29.0,
    "predict_exact_match_for_task1434_head_qa_classification": 42.0,
    "predict_exact_match_for_task143_odd-man-out_classification_generate_category": 20.0,
    "predict_exact_match_for_task1488_sarcasmdetection_headline_classification": 53.0,
    "predict_exact_match_for_task1489_sarcasmdetection_tweet_classification": 51.0,
    "predict_exact_match_for_task1500_dstc3_classification": 35.0,
    "predict_exact_match_for_task1502_hatexplain_classification": 32.0,
    "predict_exact_match_for_task1503_hatexplain_classification": 28.0,
    "predict_exact_match_for_task1505_root09_semantic_relation_classification": 48.0,
    "predict_exact_match_for_task1541_agnews_classification": 32.0,
    "predict_exact_match_for_task1548_wiqa_binary_classification": 75.0,
    "predict_exact_match_for_task1559_blimp_binary_classification": 51.0,
    "predict_exact_match_for_task1560_blimp_binary_classification": 60.0,
    "predict_exact_match_for_task1565_triviaqa_classification": 50.0,
    "predict_exact_match_for_task1568_propara_classification": 46.0,
    "predict_exact_match_for_task156_codah_classification_adversarial": 24.0,
    "predict_exact_match_for_task1573_samsum_classification": 52.0,
    "predict_exact_match_for_task1583_bless_meronym_classification": 52.0,
    "predict_exact_match_for_task1584_evalution_meronym_classification": 53.0,
    "predict_exact_match_for_task1593_yahoo_answers_topics_classification": 11.0,
    "predict_exact_match_for_task1599_smcalflow_classification": 100.0,
    "predict_exact_match_for_task1604_ethos_text_classification": 65.0,
    "predict_exact_match_for_task1605_ethos_text_classification": 68.0,
    "predict_exact_match_for_task1606_ethos_text_classification": 58.0,
    "predict_exact_match_for_task1607_ethos_text_classification": 72.0,
    "predict_exact_match_for_task1645_medical_question_pair_dataset_text_classification": 54.0,
    "predict_exact_match_for_task1661_super_glue_classification": 57.0,
    "predict_exact_match_for_task1705_ljspeech_classification": 60.0,
    "predict_exact_match_for_task1706_ljspeech_classification": 53.0,
    "predict_exact_match_for_task1712_poki_classification": 73.0,
    "predict_exact_match_for_task1720_civil_comments_toxicity_classification": 47.0,
    "predict_exact_match_for_task1721_civil_comments_obscenity_classification": 49.0,
    "predict_exact_match_for_task1722_civil_comments_threat_classification": 52.0,
    "predict_exact_match_for_task1723_civil_comments_sexuallyexplicit_classification": 49.0,
    "predict_exact_match_for_task1724_civil_comments_insult_classification": 56.0,
    "predict_exact_match_for_task1725_civil_comments_severtoxicity_classification": 55.0,
    "predict_exact_match_for_task195_sentiment140_classification": 69.0,
    "predict_exact_match_for_task209_stancedetection_classification": 51.0,
    "predict_exact_match_for_task211_logic2text_classification": 54.0,
    "predict_exact_match_for_task212_logic2text_classification": 12.0,
    "predict_exact_match_for_task227_clariq_classification": 59.0,
    "predict_exact_match_for_task248_dream_classification": 23.0,
    "predict_exact_match_for_task274_overruling_legal_classification": 57.0,
    "predict_exact_match_for_task276_enhanced_wsc_classification": 20.0,
    "predict_exact_match_for_task279_stereoset_classification_stereotype": 46.0,
    "predict_exact_match_for_task280_stereoset_classification_stereotype_type": 36.0,
    "predict_exact_match_for_task284_imdb_classification": 75.0,
    "predict_exact_match_for_task296_storycloze_correct_end_classification": 52.0,
    "predict_exact_match_for_task297_storycloze_incorrect_end_classification": 48.0,
    "predict_exact_match_for_task298_storycloze_correct_end_classification": 49.0,
    "predict_exact_match_for_task302_record_classification": 11.0,
    "predict_exact_match_for_task310_race_classification": 50.0,
    "predict_exact_match_for_task316_crows-pairs_classification_stereotype": 49.0,
    "predict_exact_match_for_task317_crows-pairs_classification_stereotype_type": 39.0,
    "predict_exact_match_for_task318_stereoset_classification_gender": 35.0,
    "predict_exact_match_for_task319_stereoset_classification_profession": 34.0,
    "predict_exact_match_for_task320_stereoset_classification_race": 37.0,
    "predict_exact_match_for_task321_stereoset_classification_religion": 41.0,
    "predict_exact_match_for_task322_jigsaw_classification_threat": 61.0,
    "predict_exact_match_for_task323_jigsaw_classification_sexually_explicit": 51.0,
    "predict_exact_match_for_task324_jigsaw_classification_disagree": 49.0,
    "predict_exact_match_for_task326_jigsaw_classification_obscene": 41.0,
    "predict_exact_match_for_task327_jigsaw_classification_toxic": 51.0,
    "predict_exact_match_for_task328_jigsaw_classification_insult": 64.0,
    "predict_exact_match_for_task333_hateeval_classification_hate_en": 56.0,
    "predict_exact_match_for_task335_hateeval_classification_aggresive_en": 50.0,
    "predict_exact_match_for_task337_hateeval_classification_individual_en": 61.0,
    "predict_exact_match_for_task340_winomt_classification_gender_pro": 69.0,
    "predict_exact_match_for_task341_winomt_classification_gender_anti": 72.0,
    "predict_exact_match_for_task342_winomt_classification_profession_pro": 67.0,
    "predict_exact_match_for_task343_winomt_classification_profession_anti": 58.0,
    "predict_exact_match_for_task346_hybridqa_classification": 53.0,
    "predict_exact_match_for_task350_winomt_classification_gender_identifiability_pro": 59.0,
    "predict_exact_match_for_task351_winomt_classification_gender_identifiability_anti": 50.0,
    "predict_exact_match_for_task353_casino_classification_negotiation_elicit_pref": 45.0,
    "predict_exact_match_for_task354_casino_classification_negotiation_no_need": 44.0,
    "predict_exact_match_for_task355_casino_classification_negotiation_other_need": 58.0,
    "predict_exact_match_for_task356_casino_classification_negotiation_self_need": 55.0,
    "predict_exact_match_for_task357_casino_classification_negotiation_small_talk": 42.0,
    "predict_exact_match_for_task358_casino_classification_negotiation_uv_part": 50.0,
    "predict_exact_match_for_task359_casino_classification_negotiation_vouch_fair": 50.0,
    "predict_exact_match_for_task363_sst2_polarity_classification": 51.0,
    "predict_exact_match_for_task364_regard_social_impact_classification": 24.0,
    "predict_exact_match_for_task379_agnews_topic_classification": 52.0,
    "predict_exact_match_for_task383_matres_classification": 37.0,
    "predict_exact_match_for_task384_socialiqa_question_classification": 31.0,
    "predict_exact_match_for_task387_semeval_2018_task3_irony_classification": 37.0,
    "predict_exact_match_for_task388_torque_token_classification": 29.0,
    "predict_exact_match_for_task400_paws_paraphrase_classification": 61.0,
    "predict_exact_match_for_task456_matres_intention_classification": 64.0,
    "predict_exact_match_for_task457_matres_conditional_classification": 72.0,
    "predict_exact_match_for_task458_matres_negation_classification": 64.0,
    "predict_exact_match_for_task459_matres_static_classification": 67.3469,
    "predict_exact_match_for_task462_qasper_classification": 31.0,
    "predict_exact_match_for_task472_haspart_classification": 95.0,
    "predict_exact_match_for_task475_yelp_polarity_classification": 52.0,
    "predict_exact_match_for_task476_cls_english_books_classification": 47.0,
    "predict_exact_match_for_task477_cls_english_dvd_classification": 50.0,
    "predict_exact_match_for_task478_cls_english_music_classification": 54.0,
    "predict_exact_match_for_task493_review_polarity_classification": 76.0,
    "predict_exact_match_for_task495_semeval_headline_classification": 52.0,
    "predict_exact_match_for_task513_argument_stance_classification": 57.0,
    "predict_exact_match_for_task514_argument_consequence_classification": 43.0,
    "predict_exact_match_for_task521_trivia_question_classification": 67.0,
    "predict_exact_match_for_task564_discofuse_classification": 11.0,
    "predict_exact_match_for_task566_circa_classification": 62.0,
    "predict_exact_match_for_task573_air_dialogue_classification": 76.0,
    "predict_exact_match_for_task575_air_dialogue_classification": 77.0,
    "predict_exact_match_for_task577_curiosity_dialogs_classification": 43.0,
    "predict_exact_match_for_task579_socialiqa_classification": 50.0,
    "predict_exact_match_for_task585_preposition_classification": 44.0,
    "predict_exact_match_for_task586_amazonfood_polarity_classification": 60.0,
    "predict_exact_match_for_task587_amazonfood_polarity_correction_classification": 54.0,
    "predict_exact_match_for_task588_amazonfood_rating_classification": 31.0,
    "predict_exact_match_for_task590_amazonfood_summary_correction_classification": 46.0,
    "predict_exact_match_for_task608_sbic_sexual_offense_binary_classification": 50.0,
    "predict_exact_match_for_task609_sbic_potentially_offense_binary_classification": 52.0,
    "predict_exact_match_for_task629_dbpedia_14_classification": 16.0,
    "predict_exact_match_for_task630_dbpedia_14_classification": 58.0,
    "predict_exact_match_for_task632_dbpedia_14_classification": 81.0,
    "predict_exact_match_for_task638_multi_woz_classification": 61.0,
    "predict_exact_match_for_task673_google_wellformed_query_classification": 65.0,
    "predict_exact_match_for_task679_hope_edi_english_text_classification": 60.0,
    "predict_exact_match_for_task681_hope_edi_malayalam_text_classification": 30.0,
    "predict_exact_match_for_task682_online_privacy_policy_text_classification": 71.0,
    "predict_exact_match_for_task746_yelp_restaurant_review_classification": 52.0,
    "predict_exact_match_for_task761_app_review_classification": 53.0,
    "predict_exact_match_for_task766_craigslist_bargains_classification": 100.0,
    "predict_exact_match_for_task767_craigslist_bargains_classification": 71.0,
    "predict_exact_match_for_task819_pec_sentiment_classification": 73.6842,
    "predict_exact_match_for_task833_poem_sentiment_classification": 81.0,
    "predict_exact_match_for_task834_mathdataset_classification": 20.0,
    "predict_exact_match_for_task843_financial_phrasebank_classification": 20.0,
    "predict_exact_match_for_task844_financial_phrasebank_classification": 40.0,
    "predict_exact_match_for_task846_pubmedqa_classification": 85.0,
    "predict_exact_match_for_task848_pubmedqa_classification": 53.0,
    "predict_exact_match_for_task854_hippocorpus_classification": 35.0,
    "predict_exact_match_for_task855_conv_ai_2_classification": 57.0,
    "predict_exact_match_for_task856_conv_ai_2_classification": 46.0,
    "predict_exact_match_for_task875_emotion_classification": 27.0,
    "predict_exact_match_for_task888_reviews_classification": 59.0,
    "predict_exact_match_for_task889_goemotions_classification": 27.0,
    "predict_exact_match_for_task900_freebase_qa_category_classification": 54.0,
    "predict_exact_match_for_task902_deceptive_opinion_spam_classification": 100.0,
    "predict_exact_match_for_task903_deceptive_opinion_spam_classification": 50.0,
    "predict_exact_match_for_task905_hate_speech_offensive_classification": 0.0,
    "predict_exact_match_for_task929_products_reviews_classification": 64.0,
    "exact_match": 50.93893418367347
}