{
    "predict_exact_match_for_task1344_glue_entailment_classification": 49.0,
    "predict_exact_match_for_task1385_anli_r1_entailment": 33.0,
    "predict_exact_match_for_task1386_anli_r2_entailment": 35.0,
    "predict_exact_match_for_task1387_anli_r3_entailment": 27.0,
    "predict_exact_match_for_task1388_cb_entailment": 42.0,
    "predict_exact_match_for_task1516_imppres_naturallanguageinference": 21.0,
    "predict_exact_match_for_task1529_scitail1.1_classification": 50.0,
    "predict_exact_match_for_task1554_scitail_classification": 48.0,
    "predict_exact_match_for_task1612_sick_label_classification": 34.0,
    "predict_exact_match_for_task1615_sick_tclassify_b_relation_a": 39.0,
    "predict_exact_match_for_task190_snli_classification": 50.0,
    "predict_exact_match_for_task199_mnli_classification": 50.0,
    "predict_exact_match_for_task200_mnli_entailment_classification": 34.0,
    "predict_exact_match_for_task201_mnli_neutral_classification": 34.0,
    "predict_exact_match_for_task202_mnli_contradiction_classification": 32.0,
    "predict_exact_match_for_task640_esnli_classification": 31.0,
    "predict_exact_match_for_task641_esnli_classification": 0.0,
    "predict_exact_match_for_task642_esnli_classification": 60.0,
    "predict_exact_match_for_task738_perspectrum_classification": 50.0,
    "predict_exact_match_for_task890_gcwd_classification": 34.0,
    "predict_exact_match_for_task935_defeasible_nli_atomic_classification": 51.0,
    "predict_exact_match_for_task936_defeasible_nli_snli_classification": 53.0,
    "predict_exact_match_for_task937_defeasible_nli_social_classification": 50.0,
    "predict_exact_match_for_task970_sherliic_causal_relationship": 54.0,
    "exact_match": 40.041666666666664
}