{
    "predict_exact_match_for_task1344_glue_entailment_classification": 47.0,
    "predict_exact_match_for_task1385_anli_r1_entailment": 34.0,
    "predict_exact_match_for_task1386_anli_r2_entailment": 36.5,
    "predict_exact_match_for_task1387_anli_r3_entailment": 31.5,
    "predict_exact_match_for_task1388_cb_entailment": 21.0,
    "predict_exact_match_for_task1516_imppres_naturallanguageinference": 19.0,
    "predict_exact_match_for_task1529_scitail1.1_classification": 50.0,
    "predict_exact_match_for_task1554_scitail_classification": 50.0,
    "predict_exact_match_for_task1612_sick_label_classification": 32.0,
    "predict_exact_match_for_task1615_sick_tclassify_b_relation_a": 34.0,
    "predict_exact_match_for_task190_snli_classification": 50.0,
    "predict_exact_match_for_task199_mnli_classification": 50.0,
    "predict_exact_match_for_task200_mnli_entailment_classification": 34.0,
    "predict_exact_match_for_task201_mnli_neutral_classification": 39.0,
    "predict_exact_match_for_task202_mnli_contradiction_classification": 36.0,
    "predict_exact_match_for_task640_esnli_classification": 31.0,
    "predict_exact_match_for_task641_esnli_classification": 29.5,
    "predict_exact_match_for_task642_esnli_classification": 55.5,
    "predict_exact_match_for_task738_perspectrum_classification": 50.0,
    "predict_exact_match_for_task890_gcwd_classification": 34.0,
    "predict_exact_match_for_task935_defeasible_nli_atomic_classification": 50.0,
    "predict_exact_match_for_task936_defeasible_nli_snli_classification": 50.0,
    "predict_exact_match_for_task937_defeasible_nli_social_classification": 51.5,
    "predict_exact_match_for_task970_sherliic_causal_relationship": 51.5,
    "exact_match": 40.291666666666664
}