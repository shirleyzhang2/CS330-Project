{
    "predict_exact_match_for_task1344_glue_entailment_classification": 51.0,
    "predict_exact_match_for_task1385_anli_r1_entailment": 39.0,
    "predict_exact_match_for_task1386_anli_r2_entailment": 37.0,
    "predict_exact_match_for_task1387_anli_r3_entailment": 33.0,
    "predict_exact_match_for_task1388_cb_entailment": 40.0,
    "predict_exact_match_for_task1516_imppres_naturallanguageinference": 31.0,
    "predict_exact_match_for_task1529_scitail1.1_classification": 50.0,
    "predict_exact_match_for_task1554_scitail_classification": 50.0,
    "predict_exact_match_for_task1612_sick_label_classification": 37.0,
    "predict_exact_match_for_task1615_sick_tclassify_b_relation_a": 34.0,
    "predict_exact_match_for_task190_snli_classification": 50.0,
    "predict_exact_match_for_task199_mnli_classification": 50.0,
    "predict_exact_match_for_task200_mnli_entailment_classification": 36.0,
    "predict_exact_match_for_task201_mnli_neutral_classification": 45.0,
    "predict_exact_match_for_task202_mnli_contradiction_classification": 39.0,
    "predict_exact_match_for_task640_esnli_classification": 36.0,
    "predict_exact_match_for_task641_esnli_classification": 35.0,
    "predict_exact_match_for_task642_esnli_classification": 60.0,
    "predict_exact_match_for_task738_perspectrum_classification": 50.0,
    "predict_exact_match_for_task890_gcwd_classification": 34.0,
    "predict_exact_match_for_task935_defeasible_nli_atomic_classification": 50.0,
    "predict_exact_match_for_task936_defeasible_nli_snli_classification": 50.0,
    "predict_exact_match_for_task937_defeasible_nli_social_classification": 53.0,
    "predict_exact_match_for_task970_sherliic_causal_relationship": 62.0,
    "exact_match": 43.833333333333336
}