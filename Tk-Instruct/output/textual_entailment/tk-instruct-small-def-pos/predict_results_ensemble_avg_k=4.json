{
    "predict_exact_match_for_task1344_glue_entailment_classification": 47.75,
    "predict_exact_match_for_task1385_anli_r1_entailment": 35.5,
    "predict_exact_match_for_task1386_anli_r2_entailment": 35.5,
    "predict_exact_match_for_task1387_anli_r3_entailment": 33.0,
    "predict_exact_match_for_task1388_cb_entailment": 28.0,
    "predict_exact_match_for_task1516_imppres_naturallanguageinference": 20.0,
    "predict_exact_match_for_task1529_scitail1.1_classification": 50.0,
    "predict_exact_match_for_task1554_scitail_classification": 50.0,
    "predict_exact_match_for_task1612_sick_label_classification": 32.25,
    "predict_exact_match_for_task1615_sick_tclassify_b_relation_a": 34.0,
    "predict_exact_match_for_task190_snli_classification": 50.0,
    "predict_exact_match_for_task199_mnli_classification": 48.5,
    "predict_exact_match_for_task200_mnli_entailment_classification": 33.25,
    "predict_exact_match_for_task201_mnli_neutral_classification": 37.75,
    "predict_exact_match_for_task202_mnli_contradiction_classification": 32.5,
    "predict_exact_match_for_task640_esnli_classification": 31.0,
    "predict_exact_match_for_task641_esnli_classification": 30.0,
    "predict_exact_match_for_task642_esnli_classification": 55.0,
    "predict_exact_match_for_task738_perspectrum_classification": 50.0,
    "predict_exact_match_for_task890_gcwd_classification": 34.0,
    "predict_exact_match_for_task935_defeasible_nli_atomic_classification": 50.0,
    "predict_exact_match_for_task936_defeasible_nli_snli_classification": 50.0,
    "predict_exact_match_for_task937_defeasible_nli_social_classification": 50.5,
    "predict_exact_match_for_task970_sherliic_causal_relationship": 53.75,
    "exact_match": 40.510416666666664
}