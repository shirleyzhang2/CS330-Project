{
    "predict_exact_match_for_task1344_glue_entailment_classification": 48.0,
    "predict_exact_match_for_task1385_anli_r1_entailment": 33.875,
    "predict_exact_match_for_task1386_anli_r2_entailment": 35.875,
    "predict_exact_match_for_task1387_anli_r3_entailment": 32.5,
    "predict_exact_match_for_task1388_cb_entailment": 26.25,
    "predict_exact_match_for_task1516_imppres_naturallanguageinference": 20.875,
    "predict_exact_match_for_task1529_scitail1.1_classification": 50.0,
    "predict_exact_match_for_task1554_scitail_classification": 50.0,
    "predict_exact_match_for_task1612_sick_label_classification": 32.25,
    "predict_exact_match_for_task1615_sick_tclassify_b_relation_a": 34.0,
    "predict_exact_match_for_task190_snli_classification": 50.0,
    "predict_exact_match_for_task199_mnli_classification": 50.0,
    "predict_exact_match_for_task200_mnli_entailment_classification": 34.125,
    "predict_exact_match_for_task201_mnli_neutral_classification": 38.375,
    "predict_exact_match_for_task202_mnli_contradiction_classification": 33.5,
    "predict_exact_match_for_task640_esnli_classification": 31.0,
    "predict_exact_match_for_task641_esnli_classification": 24.125,
    "predict_exact_match_for_task642_esnli_classification": 51.875,
    "predict_exact_match_for_task738_perspectrum_classification": 50.0,
    "predict_exact_match_for_task890_gcwd_classification": 34.0,
    "predict_exact_match_for_task935_defeasible_nli_atomic_classification": 50.0,
    "predict_exact_match_for_task936_defeasible_nli_snli_classification": 50.0,
    "predict_exact_match_for_task937_defeasible_nli_social_classification": 50.875,
    "predict_exact_match_for_task970_sherliic_causal_relationship": 54.375,
    "exact_match": 40.244791666666664
}